\documentclass[pdftex,12pt,a4paper]{article}

\usepackage[pdftex]{graphicx}

\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}

\begin{document}

\input{./title.tex}

\section{Overview}

In this section, I'm going to present two papers that talks about the image annotation. As we
all know that image annotation is becoming very popular nowadays, but automatic image annotation methods based on searching for correlations require a quality training image dataset. 

For a target image, its annotation is predicted based on a mutual similarity of the target image to the training images. One of the main problems of current methods is their low effectiveness and scalability if a relatively large-scale training dataset is used.

So starting from this point, I'm going to do some paper reading and research on this topic and gives two seperated
description about their techniques of image annotation.

\section{Image Annotation}
In this section, I'm going to talk about the technique that is used in the first paper "ANNOR: Efficient image annotation based on combining local and global features", this paper is selected from Computers \& Graphics, 2015. The main idea of this paper is a new approach named
“Automatic image aNNOtation Retriever” (ANNOR) for acquiring annotations for target images, which is
based on a combination of local and global features. According to their report, the approach gives a very good and precise result for image annotation.

\section{ANNOR}
Focusing on visual query forms, many content-based image retrieval methods and techniques have been proposed,
but they have several limitations. On one hand, in query-by-example-based
methods a query image is often absent. On the other hand, query by-sketch
approaches are too complex for common users and
a visual content interpretation of a user image concept is difficult.

Facing such a broad challenge and research focus, automatic image annotation is becoming the frontier of 
different fields such as image analysis, machine learning and information retrieval. In
present, to create a general system for automatic image annotation
based on object recognition is practically impossible. Even though many approaches have been proposed, there
have been very few breakthrough in these fields.

After carefully analyzing the problem's source, a new approach ANNOR is proposed to solve it.
ANNOR is short for the "Automatic image aNNOtation Retriever", which is retrieve the image by combining the global
and local features.

For a problem, there have been some problems that must be solved:
1) Which image representation is appropriate to describe image?
The objects in images are often occluded and appear in poor
lighting and exposure.
2) Which image features can be extracted to describe or characterize
the visual content? A feature is represented by a numerical
feature vector (descriptor), by which we are able to describe a
part of image content. In general, there are three essential
requirements for the descriptors, their degree of robustness,
discrimination ability and efficiency. The robustness represents
invariance to the geometrical changes (e.g., viewpoint, zoom,
object orientation) and noise-like signal distortions. The discrimination
maximizes difference among non-duplicates and
minimizes difference among duplicates. The feature extraction
and matching requires fast computation.
3) How much is the spatial and time complexity (computational cost) for a new method

In the paper the author proposes a method for automatic image annotation
using relatively large-scale image “training” dataset. By combining local
and global features to ensure robustness and generalization needed by
complex queries, they focus more on performance and scalability.
For indexing and clustering features, they use disk-based locality
sensitive hashing. To obtain annotation for a given target image, the
approach is based on the way how people manually annotate images.


\section{Contribution}
For this paper the most important contribution is that it gives us the key point that combines the global
and local features together, by calculating the keyword of each image using these feature, we could get the
most potential annotated keyword, and there is another equation which gives more precise result:


\subsection{Hello subsec1} Sub section here1
\subsection{Hello subsec2} Sub section here2
\paragraph{Test1}this is a test
\subparagraph{Test2} is a subparagraph
\subsection{Hello subsect3} Sub section here3
\paragraph{Huazhong University of Science and Technology} is the best University in Wuhan


\end{document}